#import libraries
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns



#read the database
df=pd.read_csv('/kaggle/input/the-boston-houseprice-data/boston.csv') #https://www.kaggle.com/datasets/fedesoriano/the-boston-houseprice-data/code

#Checking the columns
df.head() 

X = df.drop('MEDV', axis=1)  
y = df['MEDV']


corr_matrix = df.corr()  ##investigating feature importance

# Plot the heatmap
fig=plt.figure()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()
#No highly correlated features

#dividing test and train sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Defining model
xg_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)

# Defining parameters
param_grid = {
    'learning_rate': [0.0001,0.001, 0.01,0.1,0.2,0.5],
    'max_depth': [2, 5, 7,10,12,15,20,25,30,40,50],
    'n_estimators': [5,10,50,100, 200,500,1000,5000,10000]
}

# 5-fold cross-validation
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
grid_search = GridSearchCV(estimator=xg_reg, param_grid=param_grid, cv=kfold, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train_scaled, y_train)
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

best_xg_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, **best_params)
best_xg_reg.fit(X_train_scaled, y_train)
y_pred = best_xg_reg.predict(X_test_scaled )

rmse = mean_squared_error(y_test, y_pred)
print('RMSE:', rmse)

cv_results = pd.DataFrame(grid_search.cv_results_)


# Plotting hyperparameter tuning results
plt.figure(figsize=(10, 6))
plt.scatter(cv_results['param_n_estimators'], cv_results['mean_test_score'], marker='o')  
plt.xlabel('n_estimators') 
plt.ylabel('Mean Test Score')
plt.title('Hyperparameter Tuning Results')
plt.show()


plt.figure(figsize=(10, 6))
plt.scatter(cv_results['param_max_depth'], cv_results['mean_test_score'], marker='o')  
plt.xlabel('max_depth')  
plt.ylabel('Mean Test Score')
plt.title('Hyperparameter Tuning Results')
plt.show()


plt.figure(figsize=(10, 6))
plt.scatter(cv_results['param_learning_rate'], cv_results['mean_test_score'], marker='o')  
plt.xlabel('learning_rate')  
plt.ylabel('Mean Test Score')
plt.title('Hyperparameter Tuning Results')
plt.show()
