#import libraries
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
import seaborn as sns
from sklearn.metrics import r2_score


df = pd.read_csv('/kaggle/input/car-price-prediction/CarPrice_Assignment.csv')

df.head()   #Check the column 
#Divid data into separate columns 
new_columns=df.columns[0].split(',')
df[new_columns]=df.iloc[:,0].str.split(',',expand=True)
df.drop(columns=[df.columns[0]], inplace=True)

df.head()   #Check the column titles


'''
  car_ID symboling                   CarName fueltype aspiration doornumber      carbody  ... stroke compressionratio horsepower peakrpm citympg highwaympg  price
0      1         3        alfa-romero giulia      gas        std        two  convertible  ...   2.68                9        111    5000      21         27  13495
1      2         3       alfa-romero stelvio      gas        std        two  convertible  ...   2.68                9        111    5000      21         27  16500
2      3         1  alfa-romero Quadrifoglio      gas        std        two    hatchback  ...   3.47                9        154    5000      19         26  16500
3      4         2               audi 100 ls      gas        std       four        sedan  ...    3.4               10        102    5500      24         30  13950
4      5         2                audi 100ls      gas        std       four        sedan  ...    3.4                8        115    5500      18         22  17450

[5 rows x 26 columns]
'''

df.isna().sum()   ##Check for nan values

#Investigating the features

def is_numeric_column(series):
    try:
        pd.to_numeric(series)
        return True
    except ValueError:
        return False

#dividing the string and numeric columns
numeric_columns = [col for col in df.columns if is_numeric_column(df[col])]
string_columns = [col for col in df.columns if not is_numeric_column(df[col])]


df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric)

#Plotting histograms for numeric columns
for col in numeric_columns:
    plt.figure(figsize=(10, 6))
    sns.histplot(df[col], kde=False, bins=30)
    plt.title(f'Histogram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

#Plotting bar plots for string columns
for col in string_columns:
    plt.figure(figsize=(10, 6))
    sns.countplot(x=df[col])
    plt.title(f'Bar Plot of {col}')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.show()

##Plotting price vs. feature for numeric data
for col in numeric_columns:
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=df[col], y=df['price'])
    plt.title(f'Scatter Plot of {col} vs. Price')
    plt.xlabel(col)
    plt.ylabel('Price')
    plt.show()

# Applying label encoding to all string columns
label_encoders = {}
for col in string_columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

#Check the data
print(df)


#Training with all the variables
X = df.drop(columns=['car_ID',  'symboling', 'CarName','price'])
y = df['price']

#Dividing the data into test and train sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Defining the model
rf = RandomForestRegressor(random_state=42)

# Set up the parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}


grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')

# Fitting the model
grid_search.fit(X_train, y_train)

best_rf = grid_search.best_estimator_

# Predict on the test set
y_pred = best_rf.predict(X_test)

# Calculate RMSE
rmse = mean_squared_error(y_test, y_pred, squared=False)
print(f'Root Mean Squared Error: {rmse}')

# Print best parameters
print(f'Best Parameters: {grid_search.best_params_}')

##post processing
r2= r2_score(y_test, y_pred)
print("R Squared:", r2)
#R Squared: 0.9582110971590861


#Training with with only the variables that have corr.> 0.4

target_variable = 'price'
corr_matrix = df.corr()

# Extract correlations with the target variable
target_corr = corr_matrix[target_variable]

# Filtering correlations based on the threshold
strong_corrs = target_corr[(target_corr.abs() > 0.4) & (target_corr.index != target_variable)]

print("Variables with correlation > 0.4 or < -0.4 with the target variable:")
print(strong_corrs)

# Filtering the data frame to include only the selected variables
selected_columns = strong_corrs.index.tolist()
df_filtered = df[selected_columns + [target_variable]]

# Display the filtered data frame 
print(df_filtered)


X = df_filtered.drop(columns=['price'])
y = df_filtered['price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the model
rf = RandomForestRegressor(random_state=42)

# Set up the parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Set-up GridSearchCV
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')

# Fit the model
grid_search.fit(X_train, y_train)

best_rf = grid_search.best_estimator_

# Predict on the test set
y_pred = best_rf.predict(X_test)

# Calculate RMSE
rmse = mean_squared_error(y_test, y_pred, squared=False)
print(f'Root Mean Squared Error: {rmse}')

# Print best parameters
print(f'Best Parameters: {grid_search.best_params_}')

##Post-processing
r2= r2_score(y_test, y_pred)
print("R Squared:", r2)
#R Squared: 0.9508969755539398

feature_importances = best_rf.feature_importances_

# Creating a data frame for visualization
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importances')
plt.show()

#Post-processing the results
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2)
plt.title('Predicted vs Actual values')
plt.xlabel('Actual values')
plt.ylabel('Predicted values')
plt.show()

