import pandas as pd
import numpy as np
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import r2_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, KFold
from sklearn.ensemble import RandomForestRegressor
from matplotlib import pyplot as plt
import lightgbm as lgb

df=pd.read_csv("/kaggle/input/insurance/insurance.csv")
# https://www.kaggle.com/code/mahmoud2620/insurance-cost-analysis

df.head()   #check the column titles
'''
   age     sex     bmi  children smoker     region      charges
0   19  female  27.900         0    yes  southwest  16884.92400
1   18    male  33.770         1     no  southeast   1725.55230
2   28    male  33.000         3     no  southeast   4449.46200
3   33    male  22.705         0     no  northwest  21984.47061
4   32    male  28.880         0     no  northwest   3866.85520
'''

df.isna().sum()  #Check for nan values

#Changing the string columns to numeric
df['sex'] = (df['sex'] =='female').astype(int)
df['smoker'] = (df['smoker'] =='yes').astype(int)

unique_categories = df['region'].unique()
category_column = 'region'

# Initializing the LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform the category column
df[category_column] = label_encoder.fit_transform(df[category_column])

print(df.head())

'''
   age  sex     bmi  children  smoker  region      charges
0   19    1  27.900         0       1       3  16884.92400
1   18    0  33.770         1       0       2   1725.55230
2   28    0  33.000         3       0       2   4449.46200
3   33    0  22.705         0       0       1  21984.47061
4   32    0  28.880         0       0       1   3866.85520
'''

X = df.drop('charges', axis=1)
y = df['charges']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

##LightGBM

param_grid = {
    'learning_rate': [0.001, 0.01, 0.1],
    'n_estimators': [100, 200, 300],
    'num_leaves': [20, 30, 40],
    'max_depth': [3, 5, 7],
    'min_child_samples': [5, 10, 20],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
}

lgb_regressor = lgb.LGBMRegressor(random_state=42)

# Perform RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=lgb_regressor, param_distributions=param_grid, n_iter=100, scoring='neg_mean_squared_error', cv=5, verbose=False, random_state=42, n_jobs=-1)
random_search.fit(X_train, y_train)
best_params = random_search.best_params_
print("Best Hyperparameters:", best_params)

# Training the final model with the best parameters
final_gbm = lgb.LGBMRegressor(**best_params)
final_gbm.fit(X_train, y_train)
y_predgbm = final_gbm.predict(X_test)

r2gbm = r2_score(y_test, y_predgbm)
print("R Squared:", r2gbm)

#R Squared:0.8785839687023812

##RandomForestResgressor

param_grid = {
    'n_estimators': [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)],
    'max_features': ['auto', 'sqrt'],
    'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)] + [None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Create a Random Forest regressor
rf_regressor = RandomForestRegressor(random_state=42)

# Perform RandomizedSearchCV with cross-validation
random_search = RandomizedSearchCV(estimator=rf_regressor, param_distributions=param_grid, n_iter=100, scoring='neg_mean_squared_error', cv=5, verbose=False, random_state=42, n_jobs=-1)

random_search.fit(X_train, y_train)

best_params = random_search.best_params_
print("Best Hyperparameters:", best_params)

final_rf_model = RandomForestRegressor(**best_params, random_state=42)
final_rf_model.fit(X_train, y_train)

y_pred = final_rf_model.predict(X_test)

r2= r2_score(y_test, y_pred)
print("R Squared:", r2)

#R Squared: 0.878065125489163

#Presenting the results

v=np.arange(0, len(y_test))

plt.figure(figsize=(10, 6))
plt.scatter(v, y_pred, alpha=0.3, 'r')
plt.scatter(v, y_test, alpha=0.3, 'k')
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs Predicted Values")
#plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', lw=2)
plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.3)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs Predicted Values")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', lw=2)
plt.show()

